{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Neural Network for Tabular Data\n",
    "\n",
    "Publication at arXiv:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for data visualisation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from IPython import display\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch libraries for neural network model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyro libraries for Bayesian transformation\n",
    "import pyro\n",
    "from pyro.distributions import Normal, Categorical\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify data source\n",
    "BASE = '/home/odin/Data Science/Temp/PTP/'\n",
    "TRAIN_FILE = 'df_mr_train_s1k.csv'\n",
    "TEST_FILE = 'df_mr_test_s1k.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, PATH):\n",
    "        xy = np.array(pd.read_csv(PATH, encoding='utf8').values.tolist())\n",
    "        self.len = len(xy)\n",
    "        self.x_data = torch.from_numpy(xy[:, :-1].astype(np.float32))\n",
    "        self.y_data = torch.from_numpy(xy[:,-1].astype(np.int))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_data[idx], self.y_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(TabularDataset(BASE+TRAIN_FILE), \n",
    "                          batch_size=128, shuffle=True, num_workers=0)\n",
    "\n",
    "test_loader = DataLoader(TabularDataset(BASE+TEST_FILE), \n",
    "                          batch_size=128, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Neural Network of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size_1, hidden_size_2, output_size):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size_1)\n",
    "        self.fc2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
    "        self.out = nn.Linear(hidden_size_2, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.fc1(x)\n",
    "        output = self.fc2(output)\n",
    "        output = F.relu(output)\n",
    "        output = self.out(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat the neural network model\n",
    "net = NN(27, 1024, 512, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer the model to Bayesian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_softmax = nn.LogSoftmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to lift the model to Bayesian representation\n",
    "\n",
    "def model(x_data, y_data):\n",
    "    \n",
    "    fc1w_prior = Normal(loc=torch.zeros_like(net.fc1.weight), scale=torch.ones_like(net.fc1.weight))\n",
    "    fc1b_prior = Normal(loc=torch.zeros_like(net.fc1.bias), scale=torch.ones_like(net.fc1.bias))\n",
    "\n",
    "    fc2w_prior = Normal(loc=torch.zeros_like(net.fc2.weight), scale=torch.ones_like(net.fc2.weight))\n",
    "    fc2b_prior = Normal(loc=torch.zeros_like(net.fc2.bias), scale=torch.ones_like(net.fc2.bias))\n",
    "\n",
    "    outw_prior = Normal(loc=torch.zeros_like(net.out.weight), scale=torch.ones_like(net.out.weight))\n",
    "    outb_prior = Normal(loc=torch.zeros_like(net.out.bias), scale=torch.ones_like(net.out.bias))\n",
    "    \n",
    "    priors = {'fc1.weight': fc1w_prior, 'fc1.bias': fc1b_prior, 'fc2.weight': fc2w_prior, 'fc2.bias': fc2b_prior, 'out.weight': outw_prior, 'out.bias': outb_prior}\n",
    "    \n",
    "    # lift module parameters to random variables sampled from the priors\n",
    "    lifted_module = pyro.random_module(\"module\", net, priors)\n",
    "    \n",
    "    # sample a regressor (which also samples w and b)\n",
    "    lifted_reg_model = lifted_module()\n",
    "    \n",
    "    lhat = log_softmax(lifted_reg_model(x_data))\n",
    "    \n",
    "    pyro.sample(\"obs\", Categorical(logits=lhat), obs=y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softplus = torch.nn.Softplus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define guide \n",
    "\n",
    "def guide(x_data, y_data):\n",
    "    \n",
    "    # First layer weight distribution priors\n",
    "    fc1w_mu = torch.randn_like(net.fc1.weight)\n",
    "    fc1w_sigma = torch.randn_like(net.fc1.weight)\n",
    "    fc1w_mu_param = pyro.param(\"fc1w_mu\", fc1w_mu)\n",
    "    fc1w_sigma_param = softplus(pyro.param(\"fc1w_sigma\", fc1w_sigma))\n",
    "    fc1w_prior = Normal(loc=fc1w_mu_param, scale=fc1w_sigma_param)\n",
    "    \n",
    "    # First layer bias distribution priors\n",
    "    fc1b_mu = torch.randn_like(net.fc1.bias)\n",
    "    fc1b_sigma = torch.randn_like(net.fc1.bias)\n",
    "    fc1b_mu_param = pyro.param(\"fc1b_mu\", fc1b_mu)\n",
    "    fc1b_sigma_param = softplus(pyro.param(\"fc1b_sigma\", fc1b_sigma))\n",
    "    fc1b_prior = Normal(loc=fc1b_mu_param, scale=fc1b_sigma_param)\n",
    "    \n",
    "    # Second layer weight distribution priors\n",
    "    fc2w_mu = torch.randn_like(net.fc2.weight)\n",
    "    fc2w_sigma = torch.randn_like(net.fc2.weight)\n",
    "    fc2w_mu_param = pyro.param(\"fc2w_mu\", fc2w_mu)\n",
    "    fc2w_sigma_param = softplus(pyro.param(\"fc2w_sigma\", fc2w_sigma))\n",
    "    fc2w_prior = Normal(loc=fc2w_mu_param, scale=fc2w_sigma_param)\n",
    "    \n",
    "    # Second layer bias distribution priors\n",
    "    fc2b_mu = torch.randn_like(net.fc2.bias)\n",
    "    fc2b_sigma = torch.randn_like(net.fc2.bias)\n",
    "    fc2b_mu_param = pyro.param(\"fc2b_mu\", fc2b_mu)\n",
    "    fc2b_sigma_param = softplus(pyro.param(\"fc2b_sigma\", fc2b_sigma))\n",
    "    fc2b_prior = Normal(loc=fc2b_mu_param, scale=fc2b_sigma_param)\n",
    "      \n",
    "    # Output layer weight distribution priors\n",
    "    outw_mu = torch.randn_like(net.out.weight)\n",
    "    outw_sigma = torch.randn_like(net.out.weight)\n",
    "    outw_mu_param = pyro.param(\"outw_mu\", outw_mu)\n",
    "    outw_sigma_param = softplus(pyro.param(\"outw_sigma\", outw_sigma))\n",
    "    outw_prior = Normal(loc=outw_mu_param, scale=outw_sigma_param).independent(1)\n",
    "    \n",
    "    # Output layer bias distribution priors\n",
    "    outb_mu = torch.randn_like(net.out.bias)\n",
    "    outb_sigma = torch.randn_like(net.out.bias)\n",
    "    outb_mu_param = pyro.param(\"outb_mu\", outb_mu)\n",
    "    outb_sigma_param = softplus(pyro.param(\"outb_sigma\", outb_sigma))\n",
    "    outb_prior = Normal(loc=outb_mu_param, scale=outb_sigma_param)\n",
    "    priors = {'fc1.weight': fc1w_prior, 'fc1.bias': fc1b_prior, 'fc2.weight': fc2w_prior, 'fc2.bias': fc2b_prior, 'out.weight': outw_prior, 'out.bias': outb_prior}\n",
    "    \n",
    "    lifted_module = pyro.random_module(\"module\", net, priors)\n",
    "    \n",
    "    return lifted_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam({\"lr\": 0.01})\n",
    "svi = SVI(model, guide, optim, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 10\n",
    "loss = 0\n",
    "\n",
    "for j in range(num_iterations):\n",
    "    loss = 0\n",
    "    for batch_id, data in enumerate(train_loader):\n",
    "        # calculate the loss and take a gradient step\n",
    "        loss += svi.step(data[0].view(-1,27), data[1])\n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = loss / normalizer_train\n",
    "    \n",
    "    print(\"Epoch \", j, \" Loss \", total_epoch_loss_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do prediction (infrence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "def predict(x):\n",
    "    sampled_models = [guide(None, None) for _ in range(num_samples)]\n",
    "    yhats = [model(x).data for model in sampled_models]\n",
    "    mean = torch.mean(torch.stack(yhats), 0)\n",
    "    return np.argmax(mean.numpy(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction when network is forced to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction when network is forced to predict\n",
    "\n",
    "print('Prediction when network is forced to predict')\n",
    "correct = 0\n",
    "total = 0\n",
    "count = 0\n",
    "labels = None\n",
    "predicted = None\n",
    "for j, data in enumerate(test_loader):\n",
    "    images, labels = data # each data image is a row in the table\n",
    "    predicted = predict(images.view(-1,27))\n",
    "    total += labels.size(0)\n",
    "    labels = list(labels)\n",
    "    #print(predicted)\n",
    "    #print(labels)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "    count += 1\n",
    "print(\"accuracy: %d %%\" % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_performance(y_hat, y_true):\n",
    "    import datetime\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import cohen_kappa_score\n",
    "    from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "\n",
    "    predicted = np.array(y_hat)\n",
    "    actual = np.array(y_true)\n",
    "\n",
    "    tp = np.count_nonzero(predicted * actual)\n",
    "    tn = np.count_nonzero((predicted - 1) * (actual - 1))\n",
    "    fp = np.count_nonzero(predicted * (actual - 1))\n",
    "    fn = np.count_nonzero((predicted - 1) * actual)\n",
    "\n",
    "    accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    fmeasure = (2 * precision * recall) / (precision + recall)\n",
    "    cohen_kappa_score = cohen_kappa_score(predicted, actual)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(actual, predicted)\n",
    "    auc_val = auc(false_positive_rate, true_positive_rate)\n",
    "    roc_auc_val = roc_auc_score(actual, predicted)\n",
    "\n",
    "    out_string = '=========='+str(now)+'==============\\n'\n",
    "    out_string += 'Strategy:\\t' + strategy + '\\n'\n",
    "    out_string += str('Model Name:\\t' + model_name+'\\n')\n",
    "    out_string += '-------------------------------------------------' + '\\n'\n",
    "\n",
    "    out_string += 'Total Samples:\\t' + str(len(actual)) + '\\n'\n",
    "    out_string += 'Positive Samples:\\t' + str(sum(actual)) + '\\n'\n",
    "    out_string += 'Negative Samples:\\t' + str(len(actual)-sum(actual)) + '\\n'\n",
    "\n",
    "    out_string += 'True Positive:\\t' + str(tp) + '\\n'\n",
    "    out_string += 'True Negative:\\t' + str(tn) + '\\n'\n",
    "    out_string += 'False Positive:\\t' + str(fp) + '\\n'\n",
    "    out_string += 'False Negative:\\t' + str(fn) + '\\n'\n",
    "\n",
    "    out_string += 'Accuracy:\\t' + str(accuracy) + '\\n'\n",
    "    out_string += 'Precision:\\t' + str(precision) + '\\n'\n",
    "    out_string += 'Recall:\\t' + str(recall) + '\\n'\n",
    "    out_string += 'F-measure:\\t' + str(fmeasure) + '\\n'\n",
    "    out_string += 'Cohen_Kappa_Score:\\t' + str(cohen_kappa_score) + '\\n'\n",
    "    out_string += 'AUC:\\t' + str(auc_val) + '\\n'\n",
    "    out_string += 'ROC_AUC:\\t' + str(roc_auc_val) + '\\n'\n",
    "    out_string += '\\n'\n",
    "    out_string += classification_report(actual, predicted)\n",
    "    out_string += '\\n'\n",
    "    print(out_string)\n",
    "    with open(model_name+'_'+strategy+'_POP.txt', 'a+') as FO:\n",
    "        FO.write(out_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = np.array(predicted)\n",
    "y_true = np.array(labels)\n",
    "y_hat, y_true\n",
    "model_name = 'BNN'\n",
    "strategy = 'Cannot_Refuse'\n",
    "report_performance(y_hat, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Fuctionality for Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = (0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    fig, ax = plt.subplots(figsize=(1, 1))\n",
    "    ax.imshow(npimg,  cmap='gray', interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000 # Tune here\n",
    "def give_uncertainities(x):\n",
    "    sampled_models = [guide(None, None) for _ in range(num_samples)]\n",
    "    yhats = [F.log_softmax(model(x.view(-1,27)).data, 1).detach().numpy() for model in sampled_models]\n",
    "    return np.asarray(yhats)\n",
    "    #mean = torch.mean(torch.stack(yhats), 0)\n",
    "    #return np.argmax(mean, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch(images, labels, plot=True):\n",
    "    y = give_uncertainities(images)\n",
    "    predicted_for_images = 0\n",
    "    correct_predictions=0\n",
    "    y_pred = []\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "    \n",
    "        if(plot):\n",
    "            print(\"Real: \",labels[i].item())\n",
    "            fig, axs = plt.subplots(1, 2, sharey=True, figsize=(20,2))\n",
    "    \n",
    "        all_digits_prob = []\n",
    "    \n",
    "        highted_something = False\n",
    "    \n",
    "        for j in range(len(classes)):\n",
    "        \n",
    "            highlight=False\n",
    "        \n",
    "            histo = []\n",
    "            histo_exp = []\n",
    "        \n",
    "            for z in range(y.shape[0]):\n",
    "                histo.append(y[z][i][j])\n",
    "                histo_exp.append(np.exp(y[z][i][j]))\n",
    "            \n",
    "            prob = np.percentile(histo_exp, 50) #sampling median probability\n",
    "        \n",
    "            if(prob>0.7): #select if network thinks this sample is 20% chance of this being a label\n",
    "                highlight = True #possibly an answer\n",
    "        \n",
    "            all_digits_prob.append(prob)\n",
    "            \n",
    "            if(plot):\n",
    "            \n",
    "                N, bins, patches = axs[j].hist(histo, bins=8, color = \"lightgray\", lw=0,  weights=np.ones(len(histo)) / len(histo), density=False)\n",
    "                axs[j].set_title(str(j)+\" (\"+str(round(prob,2))+\")\") \n",
    "        \n",
    "            if(highlight):\n",
    "            \n",
    "                highted_something = True\n",
    "                \n",
    "                if(plot):\n",
    "\n",
    "                    # We'll color code by height, but you could use any scalar\n",
    "                    fracs = N / N.max()\n",
    "\n",
    "                    # we need to normalize the data to 0..1 for the full range of the colormap\n",
    "                    norm = colors.Normalize(fracs.min(), fracs.max())\n",
    "\n",
    "                    # Now, we'll loop through our objects and set the color of each accordingly\n",
    "                    for thisfrac, thispatch in zip(fracs, patches):\n",
    "                        color = plt.cm.viridis(norm(thisfrac))\n",
    "                        thispatch.set_facecolor(color)\n",
    "\n",
    "    \n",
    "        if(plot):\n",
    "            plt.show()\n",
    "    \n",
    "        predicted = np.argmax(all_digits_prob)\n",
    "        y_pred.append(predicted)\n",
    "    \n",
    "        if(highted_something):\n",
    "            predicted_for_images+=1\n",
    "            if(labels[i].item()==predicted):\n",
    "                if(plot):\n",
    "                    print(\"Correct\")\n",
    "                correct_predictions +=1.0\n",
    "            else:\n",
    "                if(plot):\n",
    "                    print(\"Incorrect :()\")\n",
    "        else:\n",
    "            if(plot):\n",
    "                print(\"Undecided.\")\n",
    "        \n",
    "        if(plot):\n",
    "            #imshow(images[i].squeeze())\n",
    "            pass\n",
    "        \n",
    "    \n",
    "    if(plot):\n",
    "        print(\"Summary\")\n",
    "        print(\"Total images: \",len(labels))\n",
    "        print(\"Predicted for: \",predicted_for_images)\n",
    "        print(\"Accuracy when predicted: \",correct_predictions/predicted_for_images)\n",
    "        \n",
    "    return len(labels), correct_predictions, predicted_for_images, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction when network can decide not to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction when network can decide not to predict\n",
    "\n",
    "print('Prediction when network can refuse')\n",
    "correct = 0\n",
    "total = 0\n",
    "total_predicted_for = 0\n",
    "y_hat = []\n",
    "y_true = []\n",
    "for j, data in enumerate(test_loader):\n",
    "    images, labels = data\n",
    "    \n",
    "    total_minibatch, correct_minibatch, predictions_minibatch, y_pred_batch = test_batch(images, labels, plot=False)\n",
    "    total += total_minibatch\n",
    "    correct += correct_minibatch\n",
    "    total_predicted_for += predictions_minibatch\n",
    "    y_hat.extend(y_pred_batch)\n",
    "    y_true.extend([label.item() for label in labels])\n",
    "\n",
    "print(\"Total images: \", total)\n",
    "print(\"Skipped: \", total-total_predicted_for)\n",
    "print(\"Accuracy when made predictions: %d %%\" % (100 * correct / total_predicted_for))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'BNN'\n",
    "strategy = 'Can_Refuse'\n",
    "report_performance(y_hat, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing for visual evaluation\n",
    "\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each test sample, let us show a histogram of log-probabilities for each of two classes\n",
    "Log-probability close to 0 means the probability is close to 1 (exp(0) = 1). When the label that network selected is same as the real label, it shows “Correct”. You can see from histograms that the network has a high uncertainty for both class 0 and 1 when the prediction is incorrect, i.e. where the network is undecided, the distribution of log-probabilities is wide for all classes. In the case of the accurate prediction, you will notice that the distribution for the correct class was narrow while for other class it is wide (which means the network is pretty sure of the correct class). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch(images[100:110], labels[100:110], plot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
