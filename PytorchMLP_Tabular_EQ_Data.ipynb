{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Essential Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as torch_optim\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0) # cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and preparing the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = './EQ_Dataset/'\n",
    "df_eq = pd.read_csv(BASE+'df_EQ_dataset.csv', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_eq.columns))\n",
    "# df_eq.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eq.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eq['setupYear'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eq['minDOB'] = df_eq['minDOB']/10 #convert to decade\n",
    "df_eq['minDOB'] = df_eq['minDOB'].astype(int)\n",
    "df_eq['setupYear'] = df_eq['setupYear']/10 #convert to decade\n",
    "df_eq['setupYear'] = df_eq['setupYear'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide categorical and continuous variables\n",
    "# cat_names = ['billOrder', 'billType', 'dueDayW', 'dueMonth','dueDayM', 'dueYear', 'billDayM', 'billMonth', \n",
    "#              'billRoute', 'numAccountHolders', 'hasMailAddress', 'city', 'postcode', 'incomeGroup', \n",
    "#              'wealthGroup', 'segment', 'RA_CODE_2016', 'AverageHhdSize', 'MB_CODE_2016', 'SA1_7DIGITCODE_2016', \n",
    "#              'medianPersonPerBedroom', 'minDOB', 'setupYear']\n",
    "\n",
    "cat_names = ['billOrder', 'billType', 'dueMonth', 'billRoute', 'numAccountHolders', 'hasMailAddress', 'postcode', 'incomeGroup', \n",
    "             'wealthGroup', 'AverageHhdSize', 'minDOB', 'setupYear']\n",
    "\n",
    "cont_names = ['billDuration', 'medianHhdIncWkly', 'medianMortgageWkly', 'medianRentWkly']\n",
    "\n",
    "target_name = ['billPaid']\n",
    "\n",
    "# check which variables are excluded\n",
    "exclude_names = list(set(df_eq.columns) - set(cat_names) - set(cont_names) - set(target_name))\n",
    "print(len(cat_names), len(cont_names), len(exclude_names), len(target_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eq = df_eq.drop(exclude_names, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eq = df_eq.sample(10000).reset_index(drop=True) # take a subset, remove this line after code testing\n",
    "df_eq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform LabelEncoding for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_names:\n",
    "    df_eq[col] = LabelEncoder().fit_transform(df_eq[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_eq.columns))\n",
    "df_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_eq.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making all categorical variables\n",
    "for col in cat_names:\n",
    "    df_eq[col] = df_eq[col].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classes: ', df_eq['billPaid'].unique())\n",
    "print('----Class Distrution----')\n",
    "print(df_eq['billPaid'].value_counts()/len(df_eq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Dataset into Train and Test Set\n",
    "df_train = df_eq.iloc[0 : int(len(df_eq)*0.8)]\n",
    "df_test = df_eq.iloc[int(len(df_eq)*0.8):]\n",
    "del df_eq\n",
    "len(df_train), len(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and Create Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EqDataset(Dataset):\n",
    "    def __init__(self, X, y, embedded_col_names):\n",
    "        X = X.copy()\n",
    "        self.X1 = X.loc[:,embedded_col_names].copy().values.astype(np.int64) #categorical columns\n",
    "        self.X2 = X.drop(columns=embedded_col_names).copy().values.astype(np.float32) #numerical columns\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X1[idx], self.X2[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train['billPaid'].values\n",
    "X_train = df_train.drop(['billPaid'], axis=1)\n",
    "\n",
    "y_test = df_test['billPaid'].values\n",
    "X_test = df_test.drop(['billPaid'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating train and valid datasets and dataloaders\n",
    "train_ds = EqDataset(X_train, y_train, cat_names)\n",
    "valid_ds = EqDataset(X_test, y_test, cat_names)\n",
    "\n",
    "batch_size = 10\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making device (GPU/CPU) compatible\n",
    "\n",
    "In order to make use of a GPU if available, we'll have to move our data and model to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "#         return torch.device('cpu')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "valid_dl = DeviceDataLoader(valid_dl, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Embedding sizes for Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical embedding for columns having more than two values\n",
    "colname_ncats_paris = {col_name: len(col.cat.categories) for col_name,col in df_train[cat_names].items()}\n",
    "\n",
    "# Determining size of embedding\n",
    "embedding_sizes = [(n_categories, min(10, (n_categories+1)//2)) for _, n_categories in colname_ncats_paris.items()]\n",
    "embedding_sizes # embedding_sizes :: (size of the dictionary of embeddings, the size of each embedding vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DNN (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, embedding_sizes, n_cont):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(n_categories, em_dim) for n_categories, em_dim in embedding_sizes])\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeddings) #length of all embeddings combined\n",
    "        self.n_emb, self.n_cont = n_emb, n_cont\n",
    "        \n",
    "        self.lin1 = nn.Linear(self.n_emb + self.n_cont, 100)\n",
    "        self.lin2 = nn.Linear(100, 50)\n",
    "        self.lin3 = nn.Linear(50, 2)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(self.n_cont)\n",
    "        self.bn2 = nn.BatchNorm1d(100)\n",
    "        self.bn3 = nn.BatchNorm1d(50)\n",
    "        \n",
    "        self.emb_drop = nn.Dropout(0.2)\n",
    "        self.drops = nn.Dropout(0.2)\n",
    "        \n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        x = [e(x_cat[:,i]) for i,e in enumerate(self.embeddings)]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.emb_drop(x) #categorical\n",
    "        \n",
    "        x2 = self.bn1(x_cont) #numerical\n",
    "        x = torch.cat([x, x2], 1)\n",
    "        \n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x) #layer 1\n",
    "        x = self.drops(x)\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        x = F.relu(self.lin2(x)) #layer 2\n",
    "        x = self.drops(x)\n",
    "        x = self.bn3(x)\n",
    "        \n",
    "        x = self.lin3(x) #final layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN(embedding_sizes, len(cont_names)) # use this for DNN (MLP)\n",
    "to_device(model, device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, lr = 0.001, wd = 0.0):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch_optim.Adam(parameters, lr=lr, weight_decay=wd)\n",
    "    return optimizer\n",
    "\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "lr=0.01 # learning rate \n",
    "wd=0.0 # weight decay\n",
    "optimizer = get_optimizer(model, lr = lr, wd = wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def train_model(model, train_dl):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    for x1, x2, y in train_dl:\n",
    "        batch = y.shape[0]\n",
    "        output = model(x1, x2)\n",
    "        loss = cross_entropy_loss(output, y)   \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total += batch\n",
    "        sum_loss += batch*(loss.item())\n",
    "    return sum_loss/total\n",
    "\n",
    "\n",
    "def val_loss(model, valid_dl):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    correct = 0\n",
    "    for x1, x2, y in valid_dl:\n",
    "        current_batch_size = y.shape[0]\n",
    "        out = model(x1, x2)\n",
    "        loss = cross_entropy_loss(out, y)\n",
    "        sum_loss += current_batch_size*(loss.item())\n",
    "        total += current_batch_size\n",
    "        pred = torch.max(out, 1)[1]\n",
    "        correct += (pred == y).float().sum().item()\n",
    "    print(\"valid loss %.3f and accuracy %.3f\" % (sum_loss/total, correct/total))\n",
    "    return sum_loss/total, correct/total\n",
    "\n",
    "\n",
    "def train_loop(model, epochs):\n",
    "    early_stopper = EarlyStopper(patience=3, min_delta=0)\n",
    "    for i in range(epochs): \n",
    "        loss = train_model(model, train_dl)\n",
    "        print(\"-Epoch: {} training loss: {}\".format(i, loss))\n",
    "        vloss, vaccu = val_loss(model, valid_dl)\n",
    "        print()\n",
    "        if early_stopper.early_stop(vloss):             \n",
    "            break\n",
    "        \n",
    "\n",
    "train_loop(model, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a simple Bayesian model\n",
    "\n",
    "prior_mu (Float) is the mean of prior normal distribution.\n",
    "\n",
    "prior_sigma (Float) is the sigma of prior normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rference:\n",
    "\n",
    "https://jovian.ai/aakanksha-ns/shelter-outcome"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
